# import pandas as pd
# from stable_baselines3 import PPO

# from features import generate_features
# from forex_env import ForexEnv

# df = pd.read_csv('./data/data2024.csv', index_col='timestamp', parse_dates=True)
# df = generate_features(df)
# print(df[['close', 'RCI9', 'RCI14', 'RCI21', 'Bayesian_Prob']].tail())



# train.py
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from forex_envNew import ForexEnv
from bnn_policy import BNNActorCriticPolicy

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('./data/2024min1.csv', index_col=0, parse_dates=True)
df = df.iloc[-60000:]  # å–æœ€è¿‘60000è¡Œæ•°æ®
df.reset_index(inplace=True)

# 2. ç”Ÿæˆç‰¹å¾
from featuresNew import add_features
df = add_features(df)

# ç¡®ä¿æ•°æ®å®Œæ•´
df.dropna(inplace=True)

# 3. åˆ›å»ºç¯å¢ƒ
env = ForexEnv(df, max_position=0.5)

# 4. è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æ ‡å‡†PPOï¼Œä½†ç­–ç•¥ç”±BNNæ§åˆ¶ï¼‰
# æ³¨æ„ï¼šStable-Baselines3 ä¸ç›´æ¥æ”¯æŒ MC Dropout æ¨ç†
# æˆ‘ä»¬å…ˆè®­ç»ƒä¸€ä¸ªæ ‡å‡†æ¨¡å‹ï¼Œåç»­ç”¨ BNN æ¨ç†æ—¶å†æ³¨å…¥ä¸ç¡®å®šæ€§

model = PPO(
    BNNActorCriticPolicy,
    env,
    verbose=1,
    tensorboard_log="./bnn_ppo_log/",
    learning_rate=5e-4,          # å­¦ä¹ ç‡è°ƒé«˜ï¼ŒåŠ å¿«ç­–ç•¥å˜åŒ–
    batch_size=64,
    n_steps=256,
    gamma=0.95,                  # æŠ˜æ‰£å› å­é™ä½ï¼Œé‡è§†çŸ­æœŸå›æŠ¥
    ent_coef=0.05,               # åŠ ç†µå¥–åŠ±ï¼Œé¼“åŠ±ç­–ç•¥å¤šæ ·æ€§
    clip_range=0.5,               # å…è®¸ç­–ç•¥æ›´æ–°å¹…åº¦æ›´å¤§
    device="cuda"  # å¯ç”¨GPU
)

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
model.learn(total_timesteps=200000)
model.save("bnn_ppo_forex")

print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ã€‚")

# 5. æµ‹è¯•ï¼ˆæ¼”ç¤ºä¸ç¡®å®šæ€§ä½¿ç”¨ï¼‰
# train.py
obs, _ = env.reset()
for _ in range(1000):
    # è·å–åŠ¨ä½œï¼ˆç”± PPO å†³å®šï¼‰
    action, _ = model.predict(obs, deterministic=False)

    # è·å–ä¸ç¡®å®šæ€§ï¼ˆç”¨äºåŠ¨æ€ä»“ä½ï¼‰
    feature_mean, feature_std = model.policy.predict_with_uncertainty(obs, n_samples=10)
    uncertainty_std = feature_std.mean().item()  # å–å¹³å‡ä¸ç¡®å®šæ€§

    # ä¼ å…¥ç¯å¢ƒè¿›è¡ŒåŠ¨æ€ä»“ä½ç®¡ç†
    obs, reward, done, _, info = env.step(action, uncertainty_std=uncertainty_std)

    if done:
        break
env.render()