import numpy as np
import pandas as pd
import torch as th
import os
import gym
from typing import Callable
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, StopTrainingOnNoModelImprovement
from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
from forex_envNew import ForexEnv
from bnn_policy import BNNActorCriticPolicy

# è®¾ç½®éšæœºç§å­
SEED = 42
set_random_seed(SEED, using_cuda=True)

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('./data/2024min1.csv', index_col=0, parse_dates=True)
df = df.iloc[-60000:]  # å–æœ€è¿‘60000è¡Œ
df.reset_index(inplace=True)

# 2. ç”Ÿæˆç‰¹å¾
from featuresNew import add_features
df = add_features(df)
df.dropna(inplace=True)

# 3. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
env = ForexEnv(df, max_position=0.5)
env = Monitor(env)  # æ·»åŠ ç›‘æ§

# 4. æµ‹è¯•ç¯å¢ƒ
print("ğŸ§ª æµ‹è¯•ç¯å¢ƒ...")
test_env = ForexEnv(df.iloc[:1000], max_position=0.5)  # ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•
obs, _ = test_env.reset()
for _ in range(10):
    action = test_env.action_space.sample()
    obs, reward, terminated, truncated, info = test_env.step(action)
    if terminated or truncated:
        break
print("âœ… ç¯å¢ƒæµ‹è¯•é€šè¿‡ï¼")

# 5. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
df_eval = df.iloc[-10000:].copy()
eval_env = ForexEnv(df_eval, max_position=0.5)
eval_env = Monitor(eval_env)  # æ·»åŠ ç›‘æ§

# åˆ›å»ºå‘é‡åŒ–è¯„ä¼°ç¯å¢ƒ
eval_vec_env = DummyVecEnv([lambda: eval_env])
eval_vec_env = VecNormalize(
    eval_vec_env,
    norm_obs=True,
    norm_reward=True,
    gamma=0.99,
    clip_obs=10.0,
    training=False  # å…³é”®ï¼šè®¾ç½®ä¸ºFalseè¡¨ç¤ºä¸æ›´æ–°å½’ä¸€åŒ–å‚æ•°
)

# 6. å›è°ƒå‡½æ•°
# æå‰åœæ­¢å›è°ƒï¼ˆå½“æ€§èƒ½ä¸‹é™æ—¶åœæ­¢ï¼‰
stop_callback = StopTrainingOnNoModelImprovement(
    max_no_improvement_evals=3,  # è¿ç»­3æ¬¡è¯„ä¼°æ— æ”¹è¿›
    min_evals=5,                 # è‡³å°‘5æ¬¡è¯„ä¼°åæ‰æ£€æŸ¥
    verbose=1
)

eval_callback = EvalCallback(
    eval_vec_env,  # ä½¿ç”¨å‘é‡åŒ–å½’ä¸€åŒ–ç¯å¢ƒ
    best_model_save_path="./logs/",
    log_path="./logs/",
    eval_freq=10000,
    deterministic=True,
    callback_after_eval=stop_callback,
    n_eval_episodes=1,
    verbose=1,
)

checkpoint_callback = CheckpointCallback(
    save_freq=50000,
    save_path="./checkpoints/",
    name_prefix="bnn_ppo_forex",
    verbose=1,
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¸¦é¢„çƒ­ï¼‰
def linear_schedule(initial_lr, warmup_steps=5000):
    def schedule(progress_remaining):
        # è®¡ç®—å½“å‰è®­ç»ƒæ­¥æ•°
        current_step = int((1 - progress_remaining) * total_timesteps)
        
        # é¢„çƒ­é˜¶æ®µ
        if current_step < warmup_steps:
            return initial_lr * (current_step / warmup_steps)
        
        # é¢„çƒ­åçº¿æ€§è¡°å‡
        decay_progress = (current_step - warmup_steps) / (total_timesteps - warmup_steps)
        return initial_lr * (1 - decay_progress * 0.8)  # è¡°å‡80%
    
    return schedule

# è®¾ç½®æ€»æ­¥æ•°
total_timesteps = 200000

# 7. æ„å»ºæ¨¡å‹
# åˆ›å»ºå‘é‡åŒ–è®­ç»ƒç¯å¢ƒ
train_vec_env = DummyVecEnv([lambda: env])
train_vec_env = VecNormalize(
    train_vec_env,
    norm_obs=True,
    norm_reward=True,
    gamma=0.99,
    clip_obs=10.0
)

model = PPO(
    policy=BNNActorCriticPolicy,
    env=train_vec_env,
    verbose=1,
    tensorboard_log="./bnn_ppo_log/",
    learning_rate=linear_schedule(3e-4),
    batch_size=256,
    n_steps=1024,
    gamma=0.99,
    gae_lambda=0.9,
    ent_coef=0.01,
    clip_range=0.1,
    n_epochs=5,
    max_grad_norm=0.5,
    vf_coef=0.7,
    target_kl=0.03,
    device="cuda",
    seed=SEED,
    policy_kwargs={
        'log_std_init': -1.0,  # æ›´ç¨³å®šçš„ç­–ç•¥åˆå§‹åŒ–
        'ortho_init': True,     # æ­£äº¤åˆå§‹åŒ–é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        'net_arch': [dict(pi=[256, 256], vf=[256, 256])]  # æ˜ç¡®ç½‘ç»œæ¶æ„
    }
)

print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {model.device}")
print(f"ğŸ”¢ æ€»è®­ç»ƒæ­¥æ•°: {total_timesteps}")
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

# 8. å¼€å§‹è®­ç»ƒ
model.learn(
    total_timesteps=total_timesteps,
    callback=[eval_callback, checkpoint_callback],
    tb_log_name=f"BNN_PPO_{os.getpid()}",
    progress_bar=True,
    reset_num_timesteps=False
)

# 9. ä¿å­˜æ¨¡å‹å’Œå½’ä¸€åŒ–å‚æ•°
model.save("bnn_ppo_forex_final")
train_vec_env.save("vec_normalize.pkl")  # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
eval_vec_env.save("eval_vec_normalize.pkl")  # ä¿å­˜è¯„ä¼°ç¯å¢ƒå½’ä¸€åŒ–å‚æ•°

print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ã€‚")
print(f"ğŸ“Š æŸ¥çœ‹TensorBoard: tensorboard --logdir=./bnn_ppo_log/")